{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Neural Network from scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2phZ7LTGwfIT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VFOgoG_wgub"
      },
      "source": [
        "### import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhgyv6Brweke"
      },
      "source": [
        "import numpy as np\r\n",
        "import h5py\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\r\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\r\n",
        "plt.rcParams['image.cmap'] = 'gray'\r\n",
        "\r\n",
        "%load_ext autoreload\r\n",
        "%autoreload 2\r\n",
        "\r\n",
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZWIppBowxaE"
      },
      "source": [
        "###  Single step of convolution \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od1rzgrPwo82"
      },
      "source": [
        "def conv_single_step(a_slice_prev, W, b):\r\n",
        "    \"\"\"\r\n",
        "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \r\n",
        "    of the previous layer.\r\n",
        "    \r\n",
        "    Arguments:\r\n",
        "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\r\n",
        "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\r\n",
        "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\r\n",
        "    \r\n",
        "    Returns:\r\n",
        "    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    \r\n",
        "    # Element-wise product between a_slice_prev and W. Do not add the bias yet.\r\n",
        "    s = np.multiply(a_slice_prev, W)\r\n",
        "    # Sum over all entries of the volume s.\r\n",
        "    Z = np.sum(s)\r\n",
        "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\r\n",
        "    Z = Z + float(b)\r\n",
        "    \r\n",
        "\r\n",
        "    return Z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFVa4t1cs-0h"
      },
      "source": [
        "### Convolutional Neural Networks - Forward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbWUpmgMsYsC"
      },
      "source": [
        "def conv_forward(A_prev, W, b, hparameter):\r\n",
        "\r\n",
        "      \r\n",
        "  \"\"\"\r\n",
        "  Implements the forward propagation for a convolution function\r\n",
        "    \r\n",
        "  Arguments:\r\n",
        "  A_prev -- output activations of the previous layer, \r\n",
        "      numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\r\n",
        "  W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\r\n",
        "  b -- Biases, numpy array of shape (1, 1, 1, n_C)\r\n",
        "  hparameters -- python dictionary containing \"stride\" and \"pad\"\r\n",
        "        \r\n",
        "  Returns:\r\n",
        "  Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\r\n",
        "  cache -- cache of values needed for the conv_backward() function\r\n",
        "  \"\"\"\r\n",
        "    # Retrieve dimensions from A_prev's shape   \r\n",
        "  (m, n_H_prev, n_W_prev, n_C_prev) = (A_prev.shape[0], A_prev.shape[1], A_prev.shape[2], A_prev.shape[3])\r\n",
        "    \r\n",
        "    # Retrieve dimensions from W's shape \r\n",
        "  (f, f, n_C_prev, n_C) = (W.shape[0], W.shape[1], W.shape[2], W.shape[3])\r\n",
        "    \r\n",
        "    # Retrieve information from \"hparameters\" \r\n",
        "  stride = hparameters['stride']\r\n",
        "  pad = hparameters['pad']\r\n",
        "    \r\n",
        "    # Compute the dimensions of the CONV output volume using the formula given above. \r\n",
        "  n_H = int((n_H_prev - f + 2*pad)/stride) + 1\r\n",
        "  n_W = int((n_W_prev - f + 2*pad)/stride) + 1\r\n",
        "    \r\n",
        "    # Initialize the output volume Z with zeros. \r\n",
        "  Z = np.zeros((m, n_H, n_W, n_C),dtype=float)\r\n",
        "    \r\n",
        "    # Create A_prev_pad by padding A_prev\r\n",
        "  A_prev_pad = np.pad(A_prev,((0,0),(pad,pad),(pad,pad),(0,0)), mode='constant', constant_values=(0,0))\r\n",
        "    \r\n",
        "  for i in range(m):               # loop over the batch of training examples\r\n",
        "      a_prev_pad = A_prev_pad[i,:,:,:]               # Select ith training example's padded activation\r\n",
        "      for h in range(n_H):           # loop over vertical axis of the output volume\r\n",
        "          # Find the vertical start and end of the current \"slice\" \r\n",
        "          vert_start = h*stride\r\n",
        "          vert_end = vert_start + f\r\n",
        "          \r\n",
        "          for w in range(n_W):       # loop over horizontal axis of the output volume\r\n",
        "              # Find the horizontal start and end of the current \"slice\" \r\n",
        "              horiz_start = w*stride\r\n",
        "              horiz_end = horiz_start + f\r\n",
        "              \r\n",
        "              for c in range(n_C):   # loop over channels (= #filters) of the output volume\r\n",
        "                                      \r\n",
        "                  # Use the corners to define the (3D) slice of a_prev_pad  \r\n",
        "                  a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\r\n",
        "                  \r\n",
        "                  # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. \r\n",
        "                  weights = W[:,:,:,c]\r\n",
        "                  biases = b[:,:,:,c]\r\n",
        "                  Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)\r\n",
        "                                      \r\n",
        "\r\n",
        "  # Making sure your output shape is correct\r\n",
        "  assert(Z.shape == (m, n_H, n_W, n_C))\r\n",
        "\r\n",
        "  # Save information in \"cache\" for the backpropagation\r\n",
        "  cache = (A_prev, W, b, hparameters)\r\n",
        "\r\n",
        "  return Z, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7TjgJbb1hqn"
      },
      "source": [
        "##   Pooling layer \r\n",
        "### forward pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxP-IvUE1hzt"
      },
      "source": [
        "\r\n",
        "def pool_forward(A_prev, hparameters, mode = \"max\"):\r\n",
        "    \"\"\"\r\n",
        "    Implements the forward pass of the pooling layer\r\n",
        "    \r\n",
        "    Arguments:\r\n",
        "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\r\n",
        "    hparameters -- python dictionary containing \"f\" and \"stride\"\r\n",
        "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\r\n",
        "    \r\n",
        "    Returns:\r\n",
        "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\r\n",
        "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Retrieve dimensions from the input shape\r\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\r\n",
        "    \r\n",
        "    # Retrieve hyperparameters from \"hparameters\"\r\n",
        "    f = hparameters[\"f\"]\r\n",
        "    stride = hparameters[\"stride\"]\r\n",
        "    \r\n",
        "    # Define the dimensions of the output\r\n",
        "    n_H = int(1 + (n_H_prev - f) / stride)\r\n",
        "    n_W = int(1 + (n_W_prev - f) / stride)\r\n",
        "    n_C = n_C_prev\r\n",
        "    \r\n",
        "    # Initialize output matrix A\r\n",
        "    A = np.zeros((m, n_H, n_W, n_C))              \r\n",
        "  \r\n",
        "    for i in range(m):                         # loop over the training examples\r\n",
        "        for h in range(n_H):                     # loop on the vertical axis of the output volume\r\n",
        "            # Find the vertical start and end of the current \"slice\" \r\n",
        "            vert_start = h*stride\r\n",
        "            vert_end = vert_start + f\r\n",
        "            \r\n",
        "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\r\n",
        "                # Find the vertical start and end of the current \"slice\" \r\n",
        "                horiz_start = w*stride\r\n",
        "                horiz_end = horiz_start + f\r\n",
        "                \r\n",
        "                for c in range (n_C):            # loop over the channels of the output volume\r\n",
        "                    \r\n",
        "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. \r\n",
        "                    a_prev_slice = A_prev[i,vert_start:vert_end,horiz_start:horiz_end,c]\r\n",
        "                    \r\n",
        "                    # Compute the pooling operation on the slice. \r\n",
        "                    # Use an if statement to differentiate the modes. \r\n",
        "                    # Use np.max and np.mean.\r\n",
        "                    if mode == \"max\":\r\n",
        "                        A[i, h, w, c] = np.max(a_prev_slice)\r\n",
        "                    elif mode == \"average\":\r\n",
        "                        A[i, h, w, c] = np.mean(a_prev_slice)\r\n",
        "    \r\n",
        "    \r\n",
        "    # Store the input and hparameters in \"cache\" for pool_backward()\r\n",
        "    cache = (A_prev, hparameters)\r\n",
        "    \r\n",
        "    # Making sure your output shape is correct\r\n",
        "    assert(A.shape == (m, n_H, n_W, n_C))\r\n",
        "    \r\n",
        "    return A, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRr8XP6bSS1u"
      },
      "source": [
        "### Convolutional layer backward pass "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNh8JJ_zSTEc"
      },
      "source": [
        "def conv_backward(dZ, cache):\r\n",
        "    \"\"\"\r\n",
        "    Implement the backward propagation for a convolution function\r\n",
        "    \r\n",
        "    Arguments:\r\n",
        "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\r\n",
        "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\r\n",
        "    \r\n",
        "    Returns:\r\n",
        "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\r\n",
        "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\r\n",
        "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\r\n",
        "          numpy array of shape (f, f, n_C_prev, n_C)\r\n",
        "    db -- gradient of the cost with respect to the biases of the conv layer (b)\r\n",
        "          numpy array of shape (1, 1, 1, n_C)\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "   \r\n",
        "    # Retrieve information from \"cache\"\r\n",
        "    (A_prev, W, b, hparameters) = cache\r\n",
        "    \r\n",
        "    # Retrieve dimensions from A_prev's shape\r\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape[0],A_prev.shape[1],A_prev.shape[2],A_prev.shape[3]\r\n",
        "    \r\n",
        "    # Retrieve dimensions from W's shape\r\n",
        "    (f, f, n_C_prev, n_C) = W.shape[0],W.shape[1],W.shape[2],W.shape[3]\r\n",
        "    \r\n",
        "    # Retrieve information from \"hparameters\"\r\n",
        "    stride = hparameters['stride']\r\n",
        "    pad = hparameters['pad']\r\n",
        "    \r\n",
        "    # Retrieve dimensions from dZ's shape\r\n",
        "    (m, n_H, n_W, n_C) = dZ.shape[0],dZ.shape[1],dZ.shape[2],dZ.shape[3]\r\n",
        "    \r\n",
        "    # Initialize dA_prev, dW, db with the correct shapes\r\n",
        "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev),dtype=float)                           \r\n",
        "    dW = np.zeros((f, f, n_C_prev, n_C),dtype=float)\r\n",
        "    db = np.zeros((1, 1, 1, n_C),dtype=float)\r\n",
        "\r\n",
        "    # Pad A_prev and dA_prev\r\n",
        "    A_prev_pad = np.pad(A_prev,((0,0),(pad,pad),(pad,pad),(0,0)),mode='constant',constant_values=(0,0))\r\n",
        "    dA_prev_pad = np.pad(dA_prev,((0,0),(pad,pad),(pad,pad),(0,0)),mode='constant',constant_values=(0,0))\r\n",
        "    \r\n",
        "    for i in range(m):                       # loop over the training examples\r\n",
        "        \r\n",
        "        # select ith training example from A_prev_pad and dA_prev_pad\r\n",
        "        a_prev_pad = A_prev_pad[i,:,:,:]\r\n",
        "        da_prev_pad = dA_prev_pad[i,:,:,:]\r\n",
        "        \r\n",
        "        for h in range(n_H):                   # loop over vertical axis of the output volume\r\n",
        "            for w in range(n_W):               # loop over horizontal axis of the output volume\r\n",
        "                for c in range(n_C):           # loop over the channels of the output volume\r\n",
        "                    \r\n",
        "                    # Find the corners of the current \"slice\"\r\n",
        "                    vert_start = h*stride\r\n",
        "                    vert_end = vert_start + f\r\n",
        "                    horiz_start = w*stride\r\n",
        "                    horiz_end = horiz_start + f\r\n",
        "                    \r\n",
        "                    # Use the corners to define the slice from a_prev_pad\r\n",
        "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\r\n",
        "\r\n",
        "                    # Update gradients for the window and the filter's parameters using the code formulas given above\r\n",
        "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += np.multiply(W[:,:,:,c],dZ[i, h, w, c])\r\n",
        "                    dW[:,:,:,c] += np.multiply(a_slice,dZ[i, h, w, c])\r\n",
        "                    db[:,:,:,c] += dZ[i, h, w, c]\r\n",
        "                    \r\n",
        "        # Set the ith training example's dA_prev to the unpadded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\r\n",
        "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\r\n",
        "   \r\n",
        "    \r\n",
        "    # Making sure your output shape is correct\r\n",
        "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\r\n",
        "    \r\n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUZ6PAuUS1y1"
      },
      "source": [
        "##  Pooling layer - backward pass\r\n",
        "###  Max pooling - backward pass  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pjyfnooS15E"
      },
      "source": [
        "def create_mask_from_window(x):\r\n",
        "    \"\"\"\r\n",
        "    Creates a mask from an input matrix x, to identify the max entry of x.\r\n",
        "    \r\n",
        "    Arguments:\r\n",
        "    x -- Array of shape (f, f)\r\n",
        "    \r\n",
        "    Returns:\r\n",
        "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    ### START CODE HERE ### (≈1 line)\r\n",
        "    mask = (x == np.max(x))\r\n",
        "    ### END CODE HERE ###\r\n",
        "    \r\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpmgu0ohVWeG"
      },
      "source": [
        "### Average pooling - backward pass "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2aMVRgrVSzW"
      },
      "source": [
        "def distribute_value(dz, shape):\r\n",
        "    \"\"\"\r\n",
        "    Distributes the input value in the matrix of dimension shape\r\n",
        "    \r\n",
        "    Arguments:\r\n",
        "    dz -- input scalar\r\n",
        "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\r\n",
        "    \r\n",
        "    Returns:\r\n",
        "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    \r\n",
        "    # Retrieve dimensions from shape \r\n",
        "    (n_H, n_W) = shape[0],shape[1]\r\n",
        "    \r\n",
        "    # Compute the value to distribute on the matrix \r\n",
        "    average = dz/(n_H*n_W)\r\n",
        "    \r\n",
        "    # Create a matrix where every entry is the \"average\" value \r\n",
        "    a = (np.ones((shape),dtype=float)*average)\r\n",
        "    \r\n",
        "    \r\n",
        "    return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qr6ONeraY_J"
      },
      "source": [
        "### Putting it together: Pooling backward "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQGjPMyoaZGc"
      },
      "source": [
        "def pool_backward(dA, cache, mode = \"average or max\"):\r\n",
        "    \"\"\"\r\n",
        "    Implements the backward pass of the pooling layer\r\n",
        "    \r\n",
        "    Arguments:\r\n",
        "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\r\n",
        "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \r\n",
        "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\r\n",
        "    \r\n",
        "    Returns:\r\n",
        "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "\r\n",
        "    \r\n",
        "    # Retrieve information from cache \r\n",
        "    (A_prev, hparameters) = cache\r\n",
        "    \r\n",
        "    # Retrieve hyperparameters from \"hparameters\" \r\n",
        "    stride = hparameters['stride']\r\n",
        "    f = hparameters['f']\r\n",
        "    \r\n",
        "    # Retrieve dimensions from A_prev's shape and dA's shape \r\n",
        "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape[0],A_prev.shape[1],A_prev.shape[2],A_prev.shape[3]\r\n",
        "    m, n_H, n_W, n_C = dA.shape[0],dA.shape[1],dA.shape[2],dA.shape[3]\r\n",
        "    \r\n",
        "    # Initialize dA_prev with zeros \r\n",
        "    dA_prev = np.zeros(A_prev.shape,dtype=float)\r\n",
        "    \r\n",
        "    for i in range(m):                       # loop over the training examples\r\n",
        "        \r\n",
        "        # select training example from A_prev \r\n",
        "        a_prev = A_prev[i,:,:,:]\r\n",
        "        \r\n",
        "        for h in range(n_H):                   # loop on the vertical axis\r\n",
        "            for w in range(n_W):               # loop on the horizontal axis\r\n",
        "                for c in range(n_C):           # loop over the channels (depth)\r\n",
        "                    \r\n",
        "                    # Find the corners of the current \"slice\" (≈4 lines)\r\n",
        "                    vert_start = h*stride\r\n",
        "                    vert_end = vert_start + f\r\n",
        "                    horiz_start = w*stride\r\n",
        "                    horiz_end = horiz_start + f\r\n",
        "                    \r\n",
        "                    # Compute the backward propagation in both modes.\r\n",
        "                    if mode == \"max\":\r\n",
        "                        \r\n",
        "                        # Use the corners and \"c\" to define the current slice from a_prev \r\n",
        "                        a_prev_slice = a_prev[vert_start:vert_end,horiz_start:horiz_end,c]\r\n",
        "                        # Create the mask from a_prev_slice \r\n",
        "                        mask = create_mask_from_window(a_prev_slice)\r\n",
        "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) \r\n",
        "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += np.multiply(dA[i, h, w, c],mask)\r\n",
        "                        \r\n",
        "                    elif mode == \"average\":\r\n",
        "                        \r\n",
        "                        # Get the value a from dA \r\n",
        "                        da = dA[i, h, w, c]\r\n",
        "                        # Define the shape of the filter as fxf \r\n",
        "                        shape = (f,f)\r\n",
        "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. \r\n",
        "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\r\n",
        "                        \r\n",
        "\r\n",
        "    \r\n",
        "    # Making sure your output shape is correct\r\n",
        "    assert(dA_prev.shape == A_prev.shape)\r\n",
        "    \r\n",
        "    return dA_prev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3xY9PVRyIO5"
      },
      "source": [
        "a = {'z': 55,'d': 33}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB4p7oBb42ET",
        "outputId": "5fbe43da-8541-4a36-e477-0a4873ca6a66"
      },
      "source": [
        "a['z']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj6pnOJX45aQ"
      },
      "source": [
        "b=[1,5,4,7,8,6,9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh2D1CcLZI1U",
        "outputId": "dd260a19-c565-4c08-a137-b54839b31851"
      },
      "source": [
        "b[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryAJwvKgZKAO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}